{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will go over `realDonaldTrump_tweets` and perform topic modeling. Each line in this file is a tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Load the data**\n",
    "\n",
    "Consider each tweet as a document. Load the tweets. Strip away symbols and web links in the tweets. If the tweet becomes empty string after preprocessing, then discard the tweet from analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/dsa/data/all_datasets/linguistic/realDonaldTrump_tweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each tweet as a document\n",
    "with open(file_path, 'r') as f:\n",
    "    tweets = f.read().splitlines()\n",
    "    tweets = [re.sub(r'[^\\w]|https.*\\b', ' ', t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comp to remove any string that is only spaces (empty)\n",
    "\n",
    "tweets_clean = [tweet for tweet in tweets if tweet.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Create term frequency matrix for these tweets.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(stop_words = 'english', max_df=100)\n",
    "termFrequency = countVectorizer.fit_transform(tweets_clean)\n",
    "featureNames = countVectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Apply LDA topic modeling method with 5 topics**\n",
    "\n",
    "Fit an LDA model with 5 topics on these tweets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 5)\n",
    "lda.fit(termFrequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Print the top 10 words for each of the topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0 makeamericagreatagain nytimes failing cnn way bad healthcare tickets working hard\n",
      "Topic  1 ohio florida tomorrow state obama wonderful rally movement pennsylvania watch\n",
      "Topic  2 states like enjoy united foxandfriends tonight interviewed don need bad\n",
      "Topic  3 draintheswamp americafirst live crookedhillary night bigleaguetruth minister 000 prime world\n",
      "Topic  4 honor north obamacare house korea watch women white repeal china\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic \", idx, \" \".join(featureNames[i] for i in topic.argsort()[:-10 - 1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5: Name each of the topic (No right answer)**\n",
    "\n",
    "After observing top-10 words in each topic, do these topics make sense to you? Can you name each of the topic? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My take on the topics are\n",
    "\n",
    "1) Make America Great Again\n",
    "\n",
    "2) Policies - Healthcare reform\n",
    "\n",
    "3) Debates - Florida\n",
    "\n",
    "4) Fox and Friends (news)\n",
    "\n",
    "5) Polls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6: Create a TFIDF matrix**\n",
    "\n",
    "Create TFIDF matrix for these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(tweets_clean)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6: Apply NMF topic modeling with 5 topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(n_components=5, random_state=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = NMF(n_components=5, random_state=0)\n",
    "nmf.fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7: Print the top 10 words for each of the topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0 great america make safe going honor day people today trumppence16\n",
      "Topic  1 makeamericagreatagain imwithyou erictrump lets movement lesm 6days rt nfib poll\n",
      "Topic  2 tickets join tomorrow 00pm available center 6pm tonight 7pm florida\n",
      "Topic  3 thank maga join americafirst watch florida ohio tomorrow imwithyou new\n",
      "Topic  4 amp draintheswamp hillary rt clinton trump people time realdonaldtrump media\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic \", idx, \" \".join(tfidf_feature_names[i] for i in topic.argsort()[:-10 - 1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8: Perform a comparison between the topics identified by LDA and NMF methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makeamericagreatagain 56.29160711389638\n",
      "nytimes 50.19595590545257\n",
      "failing 50.19515419836757\n",
      "cnn 48.43989549712596\n",
      "way 47.348555076279155\n",
      "bad 44.83261225330579\n",
      "healthcare 39.479150855180585\n",
      "tickets 38.41291706084292\n",
      "working 37.322650125029455\n",
      "hard 37.2693047000536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'great': 1.7212520085963903,\n",
       " 'america': 1.7118153536390415,\n",
       " 'make': 1.6293645647445902,\n",
       " 'safe': 0.3890961876697245,\n",
       " 'going': 0.2859811517064995,\n",
       " 'honor': 0.13590612162466056,\n",
       " 'day': 0.11683115476915339,\n",
       " 'people': 0.11013596341593734,\n",
       " 'today': 0.07597932743364502,\n",
       " 'trumppence16': 0.0735481618402112}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using similar method from lab\n",
    "\n",
    "# lda taking make america great\n",
    "\n",
    "topic = lda.components_[0]  \n",
    "no_top_words = 10\n",
    "\n",
    "weights_lda = {}\n",
    "for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "    print(featureNames[i], topic[i])\n",
    "    weights_lda[featureNames[i]] = topic[i]\n",
    "    \n",
    "# nmf taking make america great\n",
    "\n",
    "topic = nmf.components_[0]  \n",
    "no_top_words = 10\n",
    "\n",
    "weights_nmf = {}\n",
    "for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "    weights_nmf[tfidf_feature_names[i]] = topic[i]\n",
    "weights_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>makeamericagreatagain</td>\n",
       "      <td>56.291607</td>\n",
       "      <td>great</td>\n",
       "      <td>1.721252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>50.195956</td>\n",
       "      <td>america</td>\n",
       "      <td>1.711815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>failing</td>\n",
       "      <td>50.195154</td>\n",
       "      <td>make</td>\n",
       "      <td>1.629365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn</td>\n",
       "      <td>48.439895</td>\n",
       "      <td>safe</td>\n",
       "      <td>0.389096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>way</td>\n",
       "      <td>47.348555</td>\n",
       "      <td>going</td>\n",
       "      <td>0.285981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bad</td>\n",
       "      <td>44.832612</td>\n",
       "      <td>honor</td>\n",
       "      <td>0.135906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>healthcare</td>\n",
       "      <td>39.479151</td>\n",
       "      <td>day</td>\n",
       "      <td>0.116831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tickets</td>\n",
       "      <td>38.412917</td>\n",
       "      <td>people</td>\n",
       "      <td>0.110136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>working</td>\n",
       "      <td>37.322650</td>\n",
       "      <td>today</td>\n",
       "      <td>0.075979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hard</td>\n",
       "      <td>37.269305</td>\n",
       "      <td>trumppence16</td>\n",
       "      <td>0.073548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0          1             0         1\n",
       "0  makeamericagreatagain  56.291607         great  1.721252\n",
       "1                nytimes  50.195956       america  1.711815\n",
       "2                failing  50.195154          make  1.629365\n",
       "3                    cnn  48.439895          safe  0.389096\n",
       "4                    way  47.348555         going  0.285981\n",
       "5                    bad  44.832612         honor  0.135906\n",
       "6             healthcare  39.479151           day  0.116831\n",
       "7                tickets  38.412917        people  0.110136\n",
       "8                working  37.322650         today  0.075979\n",
       "9                   hard  37.269305  trumppence16  0.073548"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(weights_lda.items())\n",
    "df2 = pd.DataFrame(weights_nmf.items())\n",
    "\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above when I analyze the 2 systems, the lda model seemed to lean more towards tweets combining makeamericagreat and negative tweets against news stations such as CNN and NYTimes.\n",
    "The nmf model alternatively took things that seemed more positive  with words like honor and safe and today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draintheswamp 46.82920898712719\n",
      "americafirst 43.75801102863496\n",
      "live 39.81839380594453\n",
      "crookedhillary 30.1959897436008\n",
      "night 23.382520986693113\n",
      "bigleaguetruth 22.935965073333687\n",
      "minister 22.263967571772472\n",
      "000 21.856838792624796\n",
      "prime 21.52086711139623\n",
      "world 19.058042840020725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'amp': 0.6272633355501145,\n",
       " 'draintheswamp': 0.6157084913433648,\n",
       " 'hillary': 0.5150582069385732,\n",
       " 'rt': 0.49555741764462025,\n",
       " 'clinton': 0.44502729282695674,\n",
       " 'trump': 0.40160460826186667,\n",
       " 'people': 0.322511695799733,\n",
       " 'time': 0.3055033282778956,\n",
       " 'realdonaldtrump': 0.29427884485934275,\n",
       " 'media': 0.28129271417372503}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second set using the 'drain the swamp' topic\n",
    "\n",
    "# lda taking drain the swamp\n",
    "\n",
    "topic = lda.components_[3]  \n",
    "no_top_words = 10\n",
    "\n",
    "weights_lda = {}\n",
    "for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "    print(featureNames[i], topic[i])\n",
    "    weights_lda[featureNames[i]] = topic[i]\n",
    "    \n",
    "# nmf taking drain the swamp\n",
    "\n",
    "topic = nmf.components_[4]  \n",
    "no_top_words = 10\n",
    "\n",
    "weights_nmf = {}\n",
    "for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "    weights_nmf[tfidf_feature_names[i]] = topic[i]\n",
    "weights_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>draintheswamp</td>\n",
       "      <td>46.829209</td>\n",
       "      <td>amp</td>\n",
       "      <td>0.627263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>americafirst</td>\n",
       "      <td>43.758011</td>\n",
       "      <td>draintheswamp</td>\n",
       "      <td>0.615708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>live</td>\n",
       "      <td>39.818394</td>\n",
       "      <td>hillary</td>\n",
       "      <td>0.515058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crookedhillary</td>\n",
       "      <td>30.195990</td>\n",
       "      <td>rt</td>\n",
       "      <td>0.495557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>night</td>\n",
       "      <td>23.382521</td>\n",
       "      <td>clinton</td>\n",
       "      <td>0.445027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bigleaguetruth</td>\n",
       "      <td>22.935965</td>\n",
       "      <td>trump</td>\n",
       "      <td>0.401605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>minister</td>\n",
       "      <td>22.263968</td>\n",
       "      <td>people</td>\n",
       "      <td>0.322512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000</td>\n",
       "      <td>21.856839</td>\n",
       "      <td>time</td>\n",
       "      <td>0.305503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prime</td>\n",
       "      <td>21.520867</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>0.294279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>world</td>\n",
       "      <td>19.058043</td>\n",
       "      <td>media</td>\n",
       "      <td>0.281293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0          1                0         1\n",
       "0   draintheswamp  46.829209              amp  0.627263\n",
       "1    americafirst  43.758011    draintheswamp  0.615708\n",
       "2            live  39.818394          hillary  0.515058\n",
       "3  crookedhillary  30.195990               rt  0.495557\n",
       "4           night  23.382521          clinton  0.445027\n",
       "5  bigleaguetruth  22.935965            trump  0.401605\n",
       "6        minister  22.263968           people  0.322512\n",
       "7             000  21.856839             time  0.305503\n",
       "8           prime  21.520867  realdonaldtrump  0.294279\n",
       "9           world  19.058043            media  0.281293"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(weights_lda.items())\n",
    "df2 = pd.DataFrame(weights_nmf.items())\n",
    "\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this comparison, lda geared more towards america first, a little bit of hillary clinton, and looking for truth. The nmf appeared more about hillary and himself and the comparison between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11.9886px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
