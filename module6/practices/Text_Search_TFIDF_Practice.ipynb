{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Loading Text Search in Python Whoosh using TFIDF\n",
    "\n",
    "For this Practice, \n",
    "we will be creating full text search capability using Python as we did in the Lab, using TFIDF scoring. \n",
    "\n",
    "This time, our data is in the folder **`/dsa/data/all_datasets/hp`**  - but no, \n",
    "this is not Hewlett Packard documentation. \n",
    "It is something much more enchanting!\n",
    "\n",
    "Throughout the practice, reflection questions are asked. \n",
    "Take the time to answer them - consult the documentation for libraries and functions if needed, \n",
    "experiment with the code, and ask your classmates.\n",
    "\n",
    "\n",
    "## 1. Building the Whoosh Schema\n",
    "\n",
    "Import the necessary libraries and build a schema including filename, line_num and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "#TO DO: build the schema\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                line_num=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    " - Which libraries did you import and why?\n",
    "\n",
    " - Explain how you built the schema - did you use ID, TEXT, KEYWORD or STORED? \n",
    " - If so, where and why? ([Documentation available here](http://whoosh.readthedocs.io/en/latest/schema.html))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answers below \n",
    "# -----------------------\n",
    "\n",
    "From fields I imported various libraries (i.e. Schema, TEXT, ID) in order to define the fields of the files. \n",
    "After that, I imported analysis for functions related to tokenization, specifically the StemmingAnalyzer for this assignment.\n",
    "\n",
    "ID for field name to set it as one token. line_num was also ID for the same reason. Then for content I went with TEXT as it will be stored as the main block of text for the document. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Data\n",
    "\n",
    "* In the first cell, import any libraries you need, create the index in the folder `hp_index` within the practices folder, and get a writer for the index.\n",
    "* In the second cell, complete the function for loadFile\n",
    "* In the third cell, process the folder and persist your changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "import os\n",
    "from whoosh import index\n",
    "\n",
    "#TO DO: Create the index\n",
    "# Note, this clears the existing index in the directory\n",
    "os.makedirs(\"hp_index\", exist_ok=True)\n",
    "\n",
    "ix = index.create_in(\"hp_index\", schema)\n",
    "\n",
    "#TO DO: Get a writer form the created index in \n",
    "writer = ix.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Complete code below \n",
    "# -----------------------\n",
    "\n",
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    line_no = 1\n",
    "    with open(fname, 'r', encoding=\"utf-8\") as infile:\n",
    "        # TODO: create indexes for each line in the input file\n",
    "        for line in infile:\n",
    "            line=line.strip(\"\\n\")\n",
    "            writer.add_document(filename=fname, line_num = str(line_no), content=line)\n",
    "            line_no += 1 # <---------Increment after so fist line is 1. Could also change initiation to 0 with previous code\n",
    "        #-------------------------------------------------------\n",
    "        print(\"Indexed: \", fname)\n",
    "\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        # add a new line to separate folders in the output\n",
    "        print(\"\\nroot = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('root:', root, '; file:', file, '; filename:', filename)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder:  /dsa/data/all_datasets/hp\n",
      "\n",
      "root =  /dsa/data/all_datasets/hp\n",
      "root: /dsa/data/all_datasets/hp ; file: CHAPTER 1.txt ; filename: /dsa/data/all_datasets/hp/CHAPTER 1.txt\n",
      "Processing File: /dsa/data/all_datasets/hp/CHAPTER 1.txt\n",
      "Indexed:  /dsa/data/all_datasets/hp/CHAPTER 1.txt\n",
      "root: /dsa/data/all_datasets/hp ; file: CHAPTER 2.txt ; filename: /dsa/data/all_datasets/hp/CHAPTER 2.txt\n",
      "Processing File: /dsa/data/all_datasets/hp/CHAPTER 2.txt\n",
      "Indexed:  /dsa/data/all_datasets/hp/CHAPTER 2.txt\n",
      "root: /dsa/data/all_datasets/hp ; file: CHAPTER 3.txt ; filename: /dsa/data/all_datasets/hp/CHAPTER 3.txt\n",
      "Processing File: /dsa/data/all_datasets/hp/CHAPTER 3.txt\n",
      "Indexed:  /dsa/data/all_datasets/hp/CHAPTER 3.txt\n",
      "root: /dsa/data/all_datasets/hp ; file: CHAPTER 4.txt ; filename: /dsa/data/all_datasets/hp/CHAPTER 4.txt\n",
      "Processing File: /dsa/data/all_datasets/hp/CHAPTER 4.txt\n",
      "Indexed:  /dsa/data/all_datasets/hp/CHAPTER 4.txt\n",
      "root: /dsa/data/all_datasets/hp ; file: CHAPTER 5.txt ; filename: /dsa/data/all_datasets/hp/CHAPTER 5.txt\n",
      "Processing File: /dsa/data/all_datasets/hp/CHAPTER 5.txt\n",
      "Indexed:  /dsa/data/all_datasets/hp/CHAPTER 5.txt\n",
      "root: /dsa/data/all_datasets/hp ; file: CHAPTER 6.txt ; filename: /dsa/data/all_datasets/hp/CHAPTER 6.txt\n",
      "Processing File: /dsa/data/all_datasets/hp/CHAPTER 6.txt\n",
      "Indexed:  /dsa/data/all_datasets/hp/CHAPTER 6.txt\n",
      "root: /dsa/data/all_datasets/hp ; file: CHAPTER 7.txt ; filename: /dsa/data/all_datasets/hp/CHAPTER 7.txt\n",
      "Processing File: /dsa/data/all_datasets/hp/CHAPTER 7.txt\n",
      "Indexed:  /dsa/data/all_datasets/hp/CHAPTER 7.txt\n",
      "root: /dsa/data/all_datasets/hp ; file: CHAPTER 8.txt ; filename: /dsa/data/all_datasets/hp/CHAPTER 8.txt\n",
      "Processing File: /dsa/data/all_datasets/hp/CHAPTER 8.txt\n",
      "Indexed:  /dsa/data/all_datasets/hp/CHAPTER 8.txt\n"
     ]
    }
   ],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "# TODO: process the folder and persist your changes \n",
    "processFolder(writer, '/dsa/data/all_datasets/hp')\n",
    "    \n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    " - Which libraries did you import and why?\n",
    " - In loadFile, how did you get the line number for each line?\n",
    " - In loadFile, which code line adds an index for the processed line?\n",
    " - In processFolder, what does the following line do? Give an example.\n",
    "```\n",
    "filename = os.path.join(root, file)\n",
    "```\n",
    " - What code line makes sure the index get persisted? (How is it saved so it can be used?)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answers below \n",
    "# -----------------------\n",
    "\n",
    "First, grabbed os to be able to interact with the operating system. \n",
    "Then used the whoosh.index module to be able to use the functions for creating and maintaining the whoosh index for this assignment. \n",
    "\n",
    "For line_num, we just created a variable that was then incremented in the loop. We could set to either 1 or 0, would just need to update the position of the incrementation code (either before or after commiting a line).\n",
    "\n",
    "For adding the index for the processed line, that comes in:\n",
    "\n",
    "writer.add_document(filename=fname, line_num=str(line_no),content=line)\n",
    "\n",
    "The filename = os.path.join(root, file) essentially concatenates the new file that we create with the root file path so that it is searchable along the path. \n",
    "\n",
    "Finally, the code writer.commit() makes sure the index gets persisted. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Executing Queries\n",
    "* In the first cell, import any libraries you need, and find the indexes of lines where the string 'Harry' appears. Display the top 10 hits.\n",
    "* In the second cell, import any additional libraries you need, and find the indexes of lines where the string 'Harry' appears using TF-IDF as the scoring mechanism. Display the top 10 hits.\n",
    "* In the third cell, import any additional libraries you need, and use a filter to list the indexes in chapter 6 corresponding to the search string 'Harry' using TF_IDF as the scoring mechanism. Display the top 10 hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 707 4.662217912394149 0\n",
      "/dsa/data/all_datasets/hp/CHAPTER 2.txt 395 4.411890906186834 1\n",
      "/dsa/data/all_datasets/hp/CHAPTER 1.txt 96 4.235177587134104 2\n",
      "/dsa/data/all_datasets/hp/CHAPTER 2.txt 44 4.235177587134104 3\n",
      "/dsa/data/all_datasets/hp/CHAPTER 3.txt 17 4.235177587134104 4\n",
      "/dsa/data/all_datasets/hp/CHAPTER 3.txt 338 4.235177587134104 5\n",
      "/dsa/data/all_datasets/hp/CHAPTER 5.txt 915 4.235177587134104 6\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 348 4.235177587134104 7\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 423 4.235177587134104 8\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 755 4.235177587134104 9\n"
     ]
    }
   ],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "from whoosh .qparser import QueryParser\n",
    "\n",
    "#TO DO: Find the indexes of lines where the string 'Harry' appears. \n",
    "qp = QueryParser(\"content\", schema = ix.schema)\n",
    "q = qp.parse(\"Harry\")\n",
    "\n",
    "#TO DO: display the top 10 hits\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for i in results:\n",
    "        print(i['filename'], i['line_num'], i.score, i.rank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dsa/data/all_datasets/hp/CHAPTER 1.txt 96 5.0434198813672255 0\n",
      "/dsa/data/all_datasets/hp/CHAPTER 2.txt 44 5.0434198813672255 1\n",
      "/dsa/data/all_datasets/hp/CHAPTER 3.txt 17 5.0434198813672255 2\n",
      "/dsa/data/all_datasets/hp/CHAPTER 3.txt 338 5.0434198813672255 3\n",
      "/dsa/data/all_datasets/hp/CHAPTER 5.txt 915 5.0434198813672255 4\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 348 5.0434198813672255 5\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 423 5.0434198813672255 6\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 707 5.0434198813672255 7\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 755 5.0434198813672255 8\n",
      "/dsa/data/all_datasets/hp/CHAPTER 7.txt 306 5.0434198813672255 9\n"
     ]
    }
   ],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "\n",
    "#TO DO: Find the indexes of lines where the string 'Harry' appears using TF_IDF as the scoring mechanism. \n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(\"Harry\")\n",
    "\n",
    "#TO DO: display the top 10 hits\n",
    "w = scoring.BM25F(B=0.8, content_B=1.0, K1=1.5)\n",
    "\n",
    "with ix.searcher(weighting = w) as s:\n",
    "    results = s.search(q)\n",
    "    for i in results:\n",
    "        print(i['filename'], i['line_num'], i.score, i.rank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 401 6.305127613017597 0\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 707 6.305127613017597 1\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 808 6.305127613017597 2\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 5 3.1525638065087986 3\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 6 3.1525638065087986 4\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 7 3.1525638065087986 5\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 10 3.1525638065087986 6\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 13 3.1525638065087986 7\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 19 3.1525638065087986 8\n",
      "/dsa/data/all_datasets/hp/CHAPTER 6.txt 40 3.1525638065087986 9\n"
     ]
    }
   ],
   "source": [
    "# Add your code below \n",
    "# -----------------------\n",
    "\n",
    "#TO DO: import the necessary libraries for this step\n",
    "from whoosh.qparser import QueryParser # <--- since these were loaded earlier this doesn't matter...\n",
    "from whoosh import scoring\n",
    "from whoosh.query import Term\n",
    "\n",
    "#TO DO: Use a filter to list the indexes in chapter 6 corresponding to the search string 'Harry' \n",
    "# using TF_IDF as the scoring mechanism. \n",
    "\n",
    "with ix.searcher(weighting = scoring.TF_IDF()) as s:\n",
    "    qp = QueryParser(\"content\", ix.schema)\n",
    "    user_q = qp.parse(\"Harry\")\n",
    "    \n",
    "    allow_q = Term(\"filename\", \"/dsa/data/all_datasets/hp/CHAPTER 6.txt\")\n",
    "\n",
    "    #TO DO: display the top 10 hits\n",
    "    results = s.search(user_q, filter = allow_q)\n",
    "    for i in results:\n",
    "        print(i['filename'], i['line_num'], i.score, i.rank)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    " - Which libraries did you import and why?\n",
    " - What differences do you see in the results of the first two cells?\n",
    " - What do those differences mean?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your answers below \n",
    "# -----------------------\n",
    "\n",
    "QueryParser is what is used to create a query object using provided text.\n",
    "The scoring module is what includes the various scoring metrics and classes that we will use to score the searches. \n",
    "Also grabbed the Term module to get the class to show objects in the rendering chapter. \n",
    "\n",
    "For the cells, the results end up being different and ranked in different orders depending on the scoring method. This is because the second cell ranks using TF_DIF method vs the default BM25F for whoosh.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "175.625px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
