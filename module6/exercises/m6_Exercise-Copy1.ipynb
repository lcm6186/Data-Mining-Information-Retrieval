{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building and Loading Text Search in Python Whoosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='task' ></a>\n",
    "\n",
    "## Task at hand\n",
    "\n",
    "\n",
    "For this exercise, we are going to walk through the process of creating full text search capability within Python for integration into other analytical processes.\n",
    "\n",
    "You previously worked with the _`book`_ data. In this exercise, we will work with some wiki data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='build_it' ></a>\n",
    "\n",
    "## Buiding our Whoosh Schema\n",
    "\n",
    "Recall, the `book/` folder is composed of a collection of text files, each its own book chapter.\n",
    "\n",
    "In whoosh, we structure the retrieval system by defining a storage schema.\n",
    "\n",
    "From the lab with the text files:\n",
    "```\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "                )\n",
    "```\n",
    "\n",
    "This tells us we are defining records to have a `(filename, content)`\n",
    "\n",
    "For this exercise, we will be using a few Wikipedia pages for our data source.\n",
    "\n",
    "### 1) For this exercise, you should look at a few of these web pages:\n",
    "\n",
    "  * https://en.wikipedia.org/wiki/Nyctimantis\n",
    "  * https://en.wikipedia.org/wiki/Osteocephalus\n",
    "  * https://en.wikipedia.org/wiki/Osteopilus\n",
    "  \n",
    "Specifically, inspect the HTML source and the \n",
    "```HTML\n",
    "<table class=\"infobox biota\" ... </table>\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/table_inspect.png\" height=400 width=600 />\n",
    "\n",
    "\n",
    "\n",
    "**Task: You need to extend the above schema definition to collect this frog table data when available.**\n",
    "\n",
    "* Content will be the all visible text on the html page\n",
    "* Table information such as kingdom, phylum, class, order, family, subfamily, genus should be searchable "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# change this to a code cell and run if you have trouble with a locked writer\n",
    "from whoosh import writing\n",
    "writer.commit(mergetype=writing.CLEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer()),\n",
    "                # Extend the schema definition to capture relevant table data\n",
    "                kingdom=KEYWORD(stored=True),\n",
    "                phylum=KEYWORD(stored=True),\n",
    "                a_class=KEYWORD(stored=True),\n",
    "                order=KEYWORD(stored=True),\n",
    "                family=KEYWORD(stored=True),\n",
    "                subfamily=KEYWORD(stored=True),\n",
    "                genus=KEYWORD(stored=True)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "<a id='load_it' ></a>\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "For this exercise, we have created a small folder of a few Wikipedia pages under the `en.wikipedia.org/wiki` folder in the common datasets folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acris.html\t     Hylidae.html\t   Plectrohyla.html\r\n",
      "Anotheca.html\t     Hylinae.html\t   Pseudacris.html\r\n",
      "Aparasphenodon.html  Hyloscirtus.html\t   Pseudis.html\r\n",
      "Aplastodiscus.html   Hypsiboas.html\t   Ptychohyla.html\r\n",
      "Argenteohyla.html    Isthmohyla.html\t   Scarthyla.html\r\n",
      "Bokermannohyla.html  Itapotihyla.html\t   Scinax.html\r\n",
      "Bromeliohyla.html    Lysapsus.html\t   Smilisca.html\r\n",
      "Charadrahyla.html    Megastomatohyla.html  Sphaenorhynchus.html\r\n",
      "Corythomantis.html   Myersiohyla.html\t   Tepuihyla.html\r\n",
      "Dendropsophus.html   Nyctimantis.html\t   Tlalocohyla.html\r\n",
      "Duellmanohyla.html   Osteocephalus.html    Trachycephalus.html\r\n",
      "Ecnomiohyla.html     Osteopilus.html\t   Triprion.html\r\n",
      "Exerodonta.html      Phyllodytes.html\t   Xenohyla.html\r\n",
      "Hyla.html\t     Phytotriades.html\r\n"
     ]
    }
   ],
   "source": [
    "! ls /dsa/data/all_datasets/en.wikipedia.org/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You will create the _whoosh_ index files in the `modules/module6/exercises/wiki_index` folder then ingest the files.\n",
    "\n",
    "To load the data, write a python script that follow the basic crawling behavior\n",
    "\n",
    " 1. For each file/folder in the specified starting folder:\n",
    " 1. If it is a folder, recurse into folder and process contents\n",
    " 1. If it is a file, read contents and load into indexer.\n",
    " \n",
    "## Follow the lab for Python IR with whoosh to complete this exercise.\n",
    "\n",
    "### 2) Create / Initialize the whoosh index and get the `writer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 2 below this comment\"\n",
    "\n",
    "os.makedirs(\"animal_index\", exist_ok=True)  # create a directory for indexing\n",
    "\n",
    "# Note, this clears the existing index in the directory\n",
    "ix = index.create_in(\"animal_index\", schema)\n",
    "\n",
    "# Get a writer to form the created index in \n",
    "writer = ix.writer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Adapt the helper functions\n",
    "\n",
    "Note the subtle changes.\n",
    "Please adapt the code below such as provided recursive parsing of the HTML (.html) files, indexing with the Whoosh API.\n",
    "Trust no code, verify all code segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visible(element):  # return those html elements that are visible as text \n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']: #html tags\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element)): # html comments\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def pullBiota(soup):  \n",
    "        \n",
    "    data = {}   \n",
    "   \n",
    "    table = soup.find('table', class_='infobox')\n",
    "        \n",
    "    for row in table.find_all('tr'):\n",
    "        cells = row.find_all('td')  \n",
    "        \n",
    "        \n",
    "        for i in cells:\n",
    "            print(i)       \n",
    "    return data\n",
    "\n",
    "\n",
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    with open(fname, 'r') as infile:\n",
    "        html=infile.read()   # read html content ||||| I CHANGED TO html FROM content\n",
    "        \n",
    "        #-----------------------------------------\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        texts = soup.find_all(text=True)\n",
    "        \n",
    "        # Process all the visible text\n",
    "        visible_texts = filter(visible, texts)\n",
    "        #-----------------------------------------\n",
    "        \n",
    "        print(visible_texts)\n",
    "        # TODO: Assemble all visible_texts into a content string\n",
    "        # Hint: Iterate over visible_texts line by line; remove newlines; create a concatenated string\n",
    "        \n",
    "\n",
    "        # TODO: Process the \"<table class=\"infobox biota\" ... </table> data\n",
    "        infotable = pullBiota(visible_texts)\n",
    "        \n",
    "        writer.add_document(\n",
    "            filename=fname,\n",
    "            #content=content,\n",
    "            kingdom=infotable['Kingdom'],\n",
    "            phylum=infotable['Phylum'],\n",
    "            a_class=infotable['Class'],\n",
    "            order=infotable['Order'],\n",
    "            family=infotable['Family'],\n",
    "            subfamily=infotable['Subfamily'],\n",
    "            genus=infotable['Genus']\n",
    "        )\n",
    "        \n",
    "        # Write to the index\n",
    "        print(\"Indexed: \", fname)\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        print(\"root = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".html\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING THE pullBiota() FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Parse with our defined functions in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder:  /dsa/data/all_datasets/en.wikipedia.org/wiki\n",
      "root =  /dsa/data/all_datasets/en.wikipedia.org/wiki\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Acris.html\n",
      "<filter object at 0x7f0066e8e128>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'filter' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-661e113e37d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ---------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprocessFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/dsa/data/all_datasets/en.wikipedia.org/wiki'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5f83197afa4f>\u001b[0m in \u001b[0;36mprocessFolder\u001b[0;34m(writer, folder)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing File:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mloadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unhandled File\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5f83197afa4f>\u001b[0m in \u001b[0;36mloadFile\u001b[0;34m(writer, fname)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# TODO: Process the \"<table class=\"infobox biota\" ... </table> data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0minfotable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpullBiota\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         writer.add_document(\n",
      "\u001b[0;32m<ipython-input-14-5f83197afa4f>\u001b[0m in \u001b[0;36mpullBiota\u001b[0;34m(soup)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'infobox'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'filter' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Start processing the folder and commit the work\n",
    "# ---------------------------------------------------\n",
    "\n",
    "processFolder(writer, '/dsa/data/all_datasets/en.wikipedia.org/wiki')\n",
    "    \n",
    "writer.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='search_me' ></a>\n",
    "\n",
    "### 5) Executing Queries\n",
    "\n",
    "Read: \n",
    "  http://whoosh.readthedocs.io/en/latest/searching.html\n",
    "  \n",
    "Previously, we hard-coded query strings into the code cells.\n",
    "\n",
    "Now, use the `input()` function collect a query string from the user. \n",
    "Then execute the search. For this task, focus only on the `content` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Write two example queries to ensure you can search the index \n",
    "\n",
    "That is, make sure you can search on the fields you added to the index from the infobox biota table.\n",
    "\n",
    "```HTML\n",
    "<table class=\"infobox biota\" ... </table>\n",
    "```\n",
    "For this search, we will ignore `content` field and search over the other fields. We can use `MultifieldParser` to specify the fields of our interest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "from whoosh.qparser import MultifieldParser\n",
    "\n",
    "\n",
    "# OMIT CONTENT\n",
    "qp = MultifieldParser([\"Kingdom\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\"], \n",
    "                      schema=ix.schema, group=qparser.OrGroup)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "\n",
    "# OMIT CONTENT\n",
    "qp = MultifieldParser([\"Kingdom\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\"], \n",
    "                      schema=ix.schema, group=qparser.OrGroup)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE YOUR NOTEBOOK WITH ALL EXECUTED CELLS\n",
    "# Then, `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
