{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Original Dataset: https://www.kaggle.com/uciml/sms-spam-collection-dataset/home*\n",
    "\n",
    "For this exercise, there are 100 sms that have been parsed and categorized as \"Spam\" or \"Ham\". The dataframe also contains the original text message. We have converted the dataframe into a dictionary for this exercise (execute the first two cells).\n",
    "\n",
    "In the given dictionary, there are 100 entries, starting from 0 to 99 as the keys. The value for each of them is two strings, `class` and `text`. `class` contains either \"spam\" or \"ham\", based on the category of the sms, and `text` contains the original text message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/dsa/data/DSA-8410/spam.csv\", encoding='latin1')\n",
    "mini_df = df[['v1', 'v2']][:100]\n",
    "mini_df.columns = ['class', 'text']\n",
    "\n",
    "mini_df.to_csv('messages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('messages.csv')\n",
    "msgs = df.T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'class': 'ham',\n",
       "  'text': 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'},\n",
       " 1: {'class': 'ham', 'text': 'Ok lar... Joking wif u oni...'},\n",
       " 2: {'class': 'spam',\n",
       "  'text': \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"},\n",
       " 3: {'class': 'ham',\n",
       "  'text': 'U dun say so early hor... U c already then say...'},\n",
       " 4: {'class': 'ham',\n",
       "  'text': \"Nah I don't think he goes to usf, he lives around here though\"},\n",
       " 5: {'class': 'spam',\n",
       "  'text': \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\"},\n",
       " 6: {'class': 'ham',\n",
       "  'text': 'Even my brother is not like to speak with me. They treat me like aids patent.'},\n",
       " 7: {'class': 'ham',\n",
       "  'text': \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\"},\n",
       " 8: {'class': 'spam',\n",
       "  'text': 'WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'},\n",
       " 9: {'class': 'spam',\n",
       "  'text': 'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030'},\n",
       " 10: {'class': 'ham',\n",
       "  'text': \"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\"},\n",
       " 11: {'class': 'spam',\n",
       "  'text': 'SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info'},\n",
       " 12: {'class': 'spam',\n",
       "  'text': 'URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18'},\n",
       " 13: {'class': 'ham',\n",
       "  'text': \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\"},\n",
       " 14: {'class': 'ham', 'text': 'I HAVE A DATE ON SUNDAY WITH WILL!!'},\n",
       " 15: {'class': 'spam',\n",
       "  'text': 'XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL'},\n",
       " 16: {'class': 'ham', 'text': \"Oh k...i'm watching here:)\"},\n",
       " 17: {'class': 'ham',\n",
       "  'text': 'Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.'},\n",
       " 18: {'class': 'ham',\n",
       "  'text': 'Fine if thatåÕs the way u feel. ThatåÕs the way its gota b'},\n",
       " 19: {'class': 'spam',\n",
       "  'text': 'England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ì¼1.20 POBOXox36504W45WQ 16+'},\n",
       " 20: {'class': 'ham', 'text': 'Is that seriously how you spell his name?'},\n",
       " 21: {'class': 'ham',\n",
       "  'text': 'I\\x89Û÷m going to try for 2 months ha ha only joking'},\n",
       " 22: {'class': 'ham',\n",
       "  'text': 'So Ì_ pay first lar... Then when is da stock comin...'},\n",
       " 23: {'class': 'ham',\n",
       "  'text': 'Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?'},\n",
       " 24: {'class': 'ham',\n",
       "  'text': 'Ffffffffff. Alright no way I can meet up with you sooner?'},\n",
       " 25: {'class': 'ham',\n",
       "  'text': \"Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol\"},\n",
       " 26: {'class': 'ham', 'text': 'Lol your always so convincing.'},\n",
       " 27: {'class': 'ham',\n",
       "  'text': \"Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?\"},\n",
       " 28: {'class': 'ham',\n",
       "  'text': \"I'm back &amp; we're packing the car now, I'll let you know if there's room\"},\n",
       " 29: {'class': 'ham',\n",
       "  'text': 'Ahhh. Work. I vaguely remember that! What does it feel like? Lol'},\n",
       " 30: {'class': 'ham',\n",
       "  'text': \"Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us\"},\n",
       " 31: {'class': 'ham',\n",
       "  'text': \"Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? \"},\n",
       " 32: {'class': 'ham', 'text': 'K tell me anything about you.'},\n",
       " 33: {'class': 'ham',\n",
       "  'text': 'For fear of fainting with the of all that housework you just did? Quick have a cuppa'},\n",
       " 34: {'class': 'spam',\n",
       "  'text': 'Thanks for your subscription to Ringtone UK your mobile will be charged å£5/month Please confirm by replying YES or NO. If you reply NO you will not be charged'},\n",
       " 35: {'class': 'ham',\n",
       "  'text': 'Yup... Ok i go home look at the timings then i msg Ì_ again... Xuhui going to learn on 2nd may too but her lesson is at 8am'},\n",
       " 36: {'class': 'ham',\n",
       "  'text': \"Oops, I'll let you know when my roommate's done\"},\n",
       " 37: {'class': 'ham', 'text': 'I see the letter B on my car'},\n",
       " 38: {'class': 'ham', 'text': 'Anything lor... U decide...'},\n",
       " 39: {'class': 'ham',\n",
       "  'text': \"Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!\"},\n",
       " 40: {'class': 'ham',\n",
       "  'text': 'Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola'},\n",
       " 41: {'class': 'ham',\n",
       "  'text': 'Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy'},\n",
       " 42: {'class': 'spam',\n",
       "  'text': '07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow'},\n",
       " 43: {'class': 'ham', 'text': 'WHO ARE YOU SEEING?'},\n",
       " 44: {'class': 'ham',\n",
       "  'text': 'Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches...'},\n",
       " 45: {'class': 'ham', 'text': 'No calls..messages..missed calls'},\n",
       " 46: {'class': 'ham', 'text': \"Didn't you get hep b immunisation in nigeria.\"},\n",
       " 47: {'class': 'ham', 'text': 'Fair enough, anything going on?'},\n",
       " 48: {'class': 'ham',\n",
       "  'text': \"Yeah hopefully, if tyler can't do it I could maybe ask around a bit\"},\n",
       " 49: {'class': 'ham',\n",
       "  'text': \"U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers.\"},\n",
       " 50: {'class': 'ham',\n",
       "  'text': 'What you thinked about me. First time you saw me in class.'},\n",
       " 51: {'class': 'ham',\n",
       "  'text': 'A gram usually runs like  &lt;#&gt; , a half eighth is smarter though and gets you almost a whole second gram for  &lt;#&gt;'},\n",
       " 52: {'class': 'ham',\n",
       "  'text': \"K fyi x has a ride early tomorrow morning but he's crashing at our place tonight\"},\n",
       " 53: {'class': 'ham',\n",
       "  'text': 'Wow. I never realized that you were so embarassed by your accomodations. I thought you liked it, since i was doing the best i could and you always seemed so happy about \\\\the cave\\\\\". I\\'m sorry I didn\\'t and don\\'t have more to give. I\\'m sorry i offered. I\\'m sorry your room was so embarassing.\"'},\n",
       " 54: {'class': 'spam',\n",
       "  'text': 'SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV'},\n",
       " 55: {'class': 'ham',\n",
       "  'text': 'Do you know what Mallika Sherawat did yesterday? Find out now @  &lt;URL&gt;'},\n",
       " 56: {'class': 'spam',\n",
       "  'text': 'Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out! '},\n",
       " 57: {'class': 'ham', 'text': \"Sorry, I'll call later in meeting.\"},\n",
       " 58: {'class': 'ham', 'text': 'Tell where you reached'},\n",
       " 59: {'class': 'ham', 'text': 'Yes..gauti and sehwag out of odi series.'},\n",
       " 60: {'class': 'ham',\n",
       "  'text': \"Your gonna have to pick up a $1 burger for yourself on your way home. I can't even move. Pain is killing me.\"},\n",
       " 61: {'class': 'ham',\n",
       "  'text': 'Ha ha ha good joke. Girls are situation seekers.'},\n",
       " 62: {'class': 'ham', 'text': 'Its a part of checking IQ'},\n",
       " 63: {'class': 'ham',\n",
       "  'text': 'Sorry my roommates took forever, it ok if I come by now?'},\n",
       " 64: {'class': 'ham',\n",
       "  'text': 'Ok lar i double check wif da hair dresser already he said wun cut v short. He said will cut until i look nice.'},\n",
       " 65: {'class': 'spam',\n",
       "  'text': 'As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a å£1500 Bonus Prize, call 09066364589'},\n",
       " 66: {'class': 'ham',\n",
       "  'text': 'Today is \\\\song dedicated day..\\\\\" Which song will u dedicate for me? Send this to all ur valuable frnds but first rply me...\"'},\n",
       " 67: {'class': 'spam',\n",
       "  'text': 'Urgent UR awarded a complimentary trip to EuroDisinc Trav, Aco&Entry41 Or å£1000. To claim txt DIS to 87121 18+6*å£1.50(moreFrmMob. ShrAcomOrSglSuplt)10, LS1 3AJ'},\n",
       " 68: {'class': 'spam',\n",
       "  'text': 'Did you hear about the new \\\\Divorce Barbie\\\\\"? It comes with all of Ken\\'s stuff!\"'},\n",
       " 69: {'class': 'ham', 'text': 'I plane to give on this month end.'},\n",
       " 70: {'class': 'ham',\n",
       "  'text': 'Wah lucky man... Then can save money... Hee...'},\n",
       " 71: {'class': 'ham', 'text': 'Finished class where are you.'},\n",
       " 72: {'class': 'ham', 'text': 'HI BABE IM AT HOME NOW WANNA DO SOMETHING? XX'},\n",
       " 73: {'class': 'ham', 'text': 'K..k:)where are you?how did you performed?'},\n",
       " 74: {'class': 'ham', 'text': 'U can call me now...'},\n",
       " 75: {'class': 'ham', 'text': 'I am waiting machan. Call me once you free.'},\n",
       " 76: {'class': 'ham',\n",
       "  'text': 'Thats cool. i am a gentleman and will treat you with dignity and respect.'},\n",
       " 77: {'class': 'ham',\n",
       "  'text': 'I like you peoples very much:) but am very shy pa.'},\n",
       " 78: {'class': 'ham', 'text': 'Does not operate after  &lt;#&gt;  or what'},\n",
       " 79: {'class': 'ham',\n",
       "  'text': \"Its not the same here. Still looking for a job. How much do Ta's earn there.\"},\n",
       " 80: {'class': 'ham', 'text': \"Sorry, I'll call later\"},\n",
       " 81: {'class': 'ham', 'text': 'K. Did you call me just now ah? '},\n",
       " 82: {'class': 'ham', 'text': 'Ok i am on the way to home hi hi'},\n",
       " 83: {'class': 'ham', 'text': 'You will be in the place of that man'},\n",
       " 84: {'class': 'ham', 'text': 'Yup next stop.'},\n",
       " 85: {'class': 'ham',\n",
       "  'text': \"I call you later, don't have network. If urgnt, sms me.\"},\n",
       " 86: {'class': 'ham',\n",
       "  'text': \"For real when u getting on yo? I only need 2 more tickets and one more jacket and I'm done. I already used all my multis.\"},\n",
       " 87: {'class': 'ham',\n",
       "  'text': \"Yes I started to send requests to make it but pain came back so I'm back in bed. Double coins at the factory too. I gotta cash in all my nitros.\"},\n",
       " 88: {'class': 'ham', 'text': \"I'm really not up to it still tonight babe\"},\n",
       " 89: {'class': 'ham', 'text': 'Ela kano.,il download, come wen ur free..'},\n",
       " 90: {'class': 'ham',\n",
       "  'text': 'Yeah do! Don\\x89Û÷t stand to close tho- you\\x89Û÷ll catch something!'},\n",
       " 91: {'class': 'ham',\n",
       "  'text': \"Sorry to be a pain. Is it ok if we meet another night? I spent late afternoon in casualty and that means i haven't done any of y stuff42moro and that includes all my time sheets and that. Sorry. \"},\n",
       " 92: {'class': 'ham',\n",
       "  'text': 'Smile in Pleasure Smile in Pain Smile when trouble pours like Rain Smile when sum1 Hurts U Smile becoz SOMEONE still Loves to see u Smiling!!'},\n",
       " 93: {'class': 'spam',\n",
       "  'text': 'Please call our customer service representative on 0800 169 6031 between 10am-9pm as you have WON a guaranteed å£1000 cash or å£5000 prize!'},\n",
       " 94: {'class': 'ham',\n",
       "  'text': 'Havent planning to buy later. I check already lido only got 530 show in e afternoon. U finish work already?'},\n",
       " 95: {'class': 'spam',\n",
       "  'text': 'Your free ringtone is waiting to be collected. Simply text the password \\\\MIX\\\\\" to 85069 to verify. Get Usher and Britney. FML'},\n",
       " 96: {'class': 'ham', 'text': 'Watching telugu movie..wat abt u?'},\n",
       " 97: {'class': 'ham',\n",
       "  'text': 'i see. When we finish we have loads of loans to pay'},\n",
       " 98: {'class': 'ham',\n",
       "  'text': 'Hi. Wk been ok - on hols now! Yes on for a bit of a run. Forgot that i have hairdressers appointment at four so need to get home n shower beforehand. Does that cause prob for u?\\\\\"\\rham\"'},\n",
       " 99: {'class': 'ham',\n",
       "  'text': \"Please don't text me anymore. I have nothing else to say.\"}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs #<--- Nested dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.** Create a list of strings from this dictionary with the `text` values, and convert all of the strings into lowercase. Print out the first five (5) items from your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lcmhng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "# upgrading nltk\n",
    "\n",
    "#! pip install nltk -U\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "texts = []\n",
    "\n",
    "for text in msgs.values():\n",
    "    # print(text['text'])\n",
    "    texts.append(text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat...', 'ok lar... joking wif u oni...', \"free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005. text fa to 87121 to receive entry question(std txt rate)t&c's apply 08452810075over18's\", 'u dun say so early hor... u c already then say...', \"nah i don't think he goes to usf, he lives around here though\"]\n"
     ]
    }
   ],
   "source": [
    "texts = [text.lower() for text in texts]\n",
    "\n",
    "print(texts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.** Use `nltk` packages tokenize functionality on each of the strings in your list. The result should be a list of lists. Print out the first five (5) items from your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amore', 'wat', '...'], ['ok', 'lar', '...', 'joking', 'wif', 'u', 'oni', '...'], ['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 't', '&', 'c', \"'s\", 'apply', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'so', 'early', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...'], ['nah', 'i', 'do', \"n't\", 'think', 'he', 'goes', 'to', 'usf', ',', 'he', 'lives', 'around', 'here', 'though']]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "texts = [word_tokenize(text) for text in texts]\n",
    "\n",
    "print(texts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.** Remove the stopwords, punctuations and numbers from your list (list of lists). Punctuations and numbers can be checked by the function `string.punctuation` used after a string. If the result is false, you can remove that particular string from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "\n",
    "# Numbers and punctuation\n",
    "\n",
    "new_sen=[]\n",
    "\n",
    "for sen in texts:\n",
    "    words = [word for word in sen if word.isalpha()]\n",
    "    new_sen.append(words)\n",
    "    \n",
    "texts = new_sen\n",
    "\n",
    "#stop words\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "new_sen=[]\n",
    "\n",
    "for sen in texts:\n",
    "    words = [word for word in sen if word not in stop_words]\n",
    "    new_sen.append(words)\n",
    "    \n",
    "texts = new_sen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['go',\n",
       "  'jurong',\n",
       "  'point',\n",
       "  'crazy',\n",
       "  'available',\n",
       "  'bugis',\n",
       "  'n',\n",
       "  'great',\n",
       "  'world',\n",
       "  'la',\n",
       "  'e',\n",
       "  'buffet',\n",
       "  'cine',\n",
       "  'got',\n",
       "  'amore',\n",
       "  'wat'],\n",
       " ['ok', 'lar', 'joking', 'wif', 'u', 'oni'],\n",
       " ['free',\n",
       "  'entry',\n",
       "  'wkly',\n",
       "  'comp',\n",
       "  'win',\n",
       "  'fa',\n",
       "  'cup',\n",
       "  'final',\n",
       "  'tkts',\n",
       "  'may',\n",
       "  'text',\n",
       "  'fa',\n",
       "  'receive',\n",
       "  'entry',\n",
       "  'question',\n",
       "  'std',\n",
       "  'txt',\n",
       "  'rate',\n",
       "  'c',\n",
       "  'apply'],\n",
       " ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say'],\n",
       " ['nah', 'think', 'goes', 'usf', 'lives', 'around', 'though']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.** Use `nltk` packages `PorterStemmer` to stem the cleaned-text list that you got as a result of **Task 3**. Use a new variable to store the stemmed-word list, and keep the result from the **Task 3** intact. As we will use the cleaned-text list from **Task 3** in the later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "stemmed_word=[]\n",
    "\n",
    "for sen in texts:\n",
    "    stems = [porter.stem(word) for word in sen]\n",
    "    stemmed_word.append(stems)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.** Use `nltk` packages `WordNetLemmatizer` to find the lemma (or root word) from the cleaned-text list that you got as a result of **Task 3**. Consider all of the words to be a `Verb`. Use a new variable to store the lemmatized-word list, and keep the result from **Task 3** intact. As we will use the cleaned-text list from **Task 3** in the later tasks. We assume every word is a verb to make the problem easier, but we could have applied a `POS` tagger and inferred the POS for that word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "lemmatize_words=[]\n",
    "\n",
    "for sen in texts:\n",
    "    lemmas = [wordnet.lemmatize(word, pos=\"v\") for word in sen]\n",
    "    lemmatize_words.append(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.** For each lemma that we got from **Task 5**, calculate how many times they occur in all of the messages. Sort them in descending order by the number of total occurrences, and print out the top ten (10) words and their number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "# Create a dictionary using the lemmas as keys and we will default each instance as 1 for value. \n",
    "\n",
    "lemma_dict={}\n",
    "\n",
    "for sen in lemmatize_words:\n",
    "    for word in sen:\n",
    "        if word in lemma_dict.keys():\n",
    "            lemma_dict[word] += 1\n",
    "        else:\n",
    "            lemma_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Occurances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>call</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>go</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>free</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Words  Occurances\n",
       "0     u          17\n",
       "1  call          15\n",
       "2    go          11\n",
       "3   get          11\n",
       "4  free          10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict = {k: v for k, v in sorted(lemma_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "words = new_dict.keys()\n",
    "occurance = new_dict.values()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['Words'] = words\n",
    "df['Occurances'] = occurance\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7.** From the result we got from **Task 6**, remove all of the words with a length of 1 and select the top hundred (100) most frequent terms from it. We will use this list of words in our next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "#This is kinda pointless if we are only grabbing the top 100 and it's already sorted\n",
    "df_100 = df[df.Occurances != 1]\n",
    "df_100 = df.head(100)\n",
    "\n",
    "df_100\n",
    "\n",
    "top_100 = df_100['Words'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.** For each message (use the lemma-list we created for **Task 5**), calculate the number of times each word from **Task 7** (top-100 words) occurs in that message. \n",
    "Create a **Data-Matrix** using your calculations. Each row should correspond to a message, and each column should correspond to a word from the list we got in **Task 7**. Each cell should correspond to how many times that particular word (from column) occurs for that specific message (from row).\n",
    "\n",
    "You can use Pandas-DataFrame to store your **Data-Matrix**. Print the first five rows of the Data-Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a blank dictionary to hold the values\n",
    "# Will update this and append to created dataframe\n",
    "\n",
    "word_dict = {}\n",
    "for word in top_100:\n",
    "    word_dict[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "# Start with an empty data frame\n",
    "word_df = pd.DataFrame(columns=top_100)\n",
    "\n",
    "\n",
    "for sen in lemmatize_words:\n",
    "    for word in sen:\n",
    "        if word in top_100:\n",
    "            # Update the word dictionary value by 1 if the word is in the sentence\n",
    "            word_dict[word] += 1\n",
    "    # Now we append the dictionary to the data frame and reset each value pair to 0\n",
    "    word_df = word_df.append(word_dict, ignore_index=True)\n",
    "    \n",
    "    for key in word_dict.keys():\n",
    "        word_dict[key] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>call</th>\n",
       "      <th>go</th>\n",
       "      <th>get</th>\n",
       "      <th>free</th>\n",
       "      <th>like</th>\n",
       "      <th>ok</th>\n",
       "      <th>sorry</th>\n",
       "      <th>txt</th>\n",
       "      <th>already</th>\n",
       "      <th>...</th>\n",
       "      <th>value</th>\n",
       "      <th>network</th>\n",
       "      <th>months</th>\n",
       "      <th>update</th>\n",
       "      <th>gon</th>\n",
       "      <th>stuff</th>\n",
       "      <th>anymore</th>\n",
       "      <th>enough</th>\n",
       "      <th>today</th>\n",
       "      <th>urgent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   u call go get free like ok sorry txt already  ... value network months  \\\n",
       "0  0    0  1   1    0    0  0     0   0       0  ...     0       0      0   \n",
       "1  1    0  0   0    0    0  1     0   0       0  ...     0       0      0   \n",
       "2  0    0  0   0    1    0  0     0   1       0  ...     0       0      0   \n",
       "3  2    0  0   0    0    0  0     0   0       1  ...     0       0      0   \n",
       "4  0    0  1   0    0    0  0     0   0       0  ...     0       0      0   \n",
       "\n",
       "  update gon stuff anymore enough today urgent  \n",
       "0      0   0     0       0      0     0      0  \n",
       "1      0   0     0       0      0     0      0  \n",
       "2      0   0     0       0      0     0      0  \n",
       "3      0   0     0       0      0     0      0  \n",
       "4      0   0     0       0      0     0      0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'get', 'amore', 'wat']\n",
      "['ok', 'lar', 'joke', 'wif', 'u', 'oni']\n",
      "['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply']\n",
      "['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say']\n",
      "['nah', 'think', 'go', 'usf', 'live', 'around', 'though']\n"
     ]
    }
   ],
   "source": [
    "# Confirming with first five sentences\n",
    "\n",
    "for sen in lemmatize_words[:5]:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
