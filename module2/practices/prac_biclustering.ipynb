{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biclustering\n",
    "\n",
    "In this exercise, we will explore the biclustering method on a spam dataset. Perform the following tasks: \n",
    "\n",
    "1. Create feature matrix using the raw text using TFIDF vectorizer \n",
    "    - you can use the one that is defined in the lab\n",
    "2. Remove empty rows/columns from the TFIDF matrix\n",
    "3. Apply spectral coclustering method on this matrix; extract two biclusters\n",
    "4. Compare results against k-means\n",
    "5. Identify the combination of spam and ham as well as important words in each bicluster\n",
    "    - do these biclusters make sense in terms of topic mixture and words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from time import time\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralCoclustering,SpectralBiclustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Original Dataset: https://www.kaggle.com/uciml/sms-spam-collection-dataset/home*\n",
    "\n",
    "For this exercise, there are 100 sms that have been parsed and categorized as \"Spam\" or \"Ham\". The dataframe also contains the original text message. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/dsa/data/DSA-8410/spam.csv\", encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = df[['v1', 'v2']][:100]\n",
    "df_sub.columns = ['class', 'text']\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_sub['class']\n",
    "categories =['spam','ham']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Create a TFIDF matrix from the raw text. You can use `NumberNormalizingVectorizer` defined in the lab. Use min_df=3 and max_features = 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_normalizer(tokens):\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = NumberNormalizingVectorizer(stop_words='english', min_df=3, max_features=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df_sub['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Remove empty rows (i.e. all values are 0) from the feature matrix. Also, update y_true by removing the labels corresponding to the empty rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#NUMBER</th>\n",
       "      <th>cash</th>\n",
       "      <th>claim</th>\n",
       "      <th>customer</th>\n",
       "      <th>did</th>\n",
       "      <th>didn</th>\n",
       "      <th>does</th>\n",
       "      <th>don</th>\n",
       "      <th>feel</th>\n",
       "      <th>finish</th>\n",
       "      <th>...</th>\n",
       "      <th>tell</th>\n",
       "      <th>text</th>\n",
       "      <th>tonight</th>\n",
       "      <th>txt</th>\n",
       "      <th>ur</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>won</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.866189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    #NUMBER  cash  claim  customer  did  didn  does  don  feel  finish  ...  \\\n",
       "0  0.000000   0.0    0.0       0.0  0.0   0.0   0.0  0.0   0.0     0.0  ...   \n",
       "1  0.000000   0.0    0.0       0.0  0.0   0.0   0.0  0.0   0.0     0.0  ...   \n",
       "2  0.866189   0.0    0.0       0.0  0.0   0.0   0.0  0.0   0.0     0.0  ...   \n",
       "3  0.000000   0.0    0.0       0.0  0.0   0.0   0.0  0.0   0.0     0.0  ...   \n",
       "4  0.000000   0.0    0.0       0.0  0.0   0.0   0.0  1.0   0.0     0.0  ...   \n",
       "\n",
       "   tell      text  tonight       txt   ur  want  way  won  yeah  yes  \n",
       "0   0.0  0.000000      0.0  0.000000  0.0   0.0  0.0  0.0   0.0  0.0  \n",
       "1   0.0  0.000000      0.0  0.000000  0.0   0.0  0.0  0.0   0.0  0.0  \n",
       "2   0.0  0.322157      0.0  0.279525  0.0   0.0  0.0  0.0   0.0  0.0  \n",
       "3   0.0  0.000000      0.0  0.000000  0.0   0.0  0.0  0.0   0.0  0.0  \n",
       "4   0.0  0.000000      0.0  0.000000  0.0   0.0  0.0  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.DataFrame.sparse.from_spmatrix(X, columns = vectorizer.get_feature_names())\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty = np.logical_not(df_new.sum(axis=1) == 0)\n",
    "df_clean = df_new[non_empty]\n",
    "\n",
    "y_true = y_true[non_empty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Define and fit a spectral coclustering model; use n_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpectralCoclustering(n_clusters=2, random_state=0, svd_method='arpack')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocluster = SpectralCoclustering(n_clusters=2, svd_method='arpack', random_state=0)\n",
    "\n",
    "cocluster.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4. Compare the peformance against k-Means using V-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cocluster = cocluster.row_labels_\n",
    "y_cocluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V-measure: 0.1300\n"
     ]
    }
   ],
   "source": [
    "v_score = v_measure_score(y_cocluster, y_true)\n",
    "print(f\"V-measure: {v_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "y_kmeans = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V-measure: 0.5117\n"
     ]
    }
   ],
   "source": [
    "v_score = v_measure_score(y_kmeans, y_true)\n",
    "print(f\"V-measure: {v_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5. Identify the combination of spam and ham as well as important words in each bicluster. Do these biclusters make sense in terms of topic mixture and words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "document_names = y_true.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions from labs\n",
    "\n",
    "def bicluster_ncut(i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # Note: the following is identical to X[rows[:, np.newaxis],\n",
    "    # cols].sum() but much faster in scipy <= 0.16\n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = (X[row_complement][:, cols].sum() +\n",
    "           X[rows][:, col_complement].sum())\n",
    "    return cut / weight\n",
    "\n",
    "\n",
    "def most_common(d):\n",
    "    \"\"\"Items of a defaultdict(int) with the highest values.\n",
    "\n",
    "    Like Counter.most_common in Python >=2.7.\n",
    "    \"\"\"\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bicluster_ncuts = list(bicluster_ncut(i)\n",
    "                       for i in range(len(categories)))\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best biclusters:\n",
      "----------------\n",
      "bicluster 0 : 60 documents, 36 words\n",
      "categories   : 72% ham, 28% spam\n",
      "words        : #NUMBER, like, free, sorry, ll, don, yes, tell, know, man\n",
      "\n",
      "bicluster 1 : 20 documents, 10 words\n",
      "categories   : 100% ham\n",
      "words        : just, way, lar, tonight, going, home, lol, hi, ok, prize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = defaultdict(int)\n",
    "    for i in cluster_docs:\n",
    "        counter[document_names[i]] += 1\n",
    "    cat_string = \", \".join(\"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n",
    "                           for name, c in most_common(counter)[:3])\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n",
    "    word_col = X[:, cluster_words]\n",
    "    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -\n",
    "                           word_col[out_of_cluster_docs, :].sum(axis=0))\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(feature_names[cluster_words[i]]\n",
    "                           for i in word_scores.argsort()[:-11:-1])\n",
    "\n",
    "    print(\"bicluster {} : {} documents, {} words\".format(\n",
    "        idx, n_rows, n_cols))\n",
    "    print(\"categories   : {}\".format(cat_string))\n",
    "    print(\"words        : {}\\n\".format(', '.join(important_words)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Overall I believe these biclusters are good, however I am surprised to see the first one as a split of around 2/3 hamd and 1/3 spam as I would have thought maybe they would be more even. Nevertheless some of the words included in that one such as 'free', 'like', can be within both so it makes sense that those would be in the same cluster. \n",
    "\n",
    "For the second cluster however, seeing as it is 100% ham, I am surprised that the word 'prize' is represented as I would most likely categorize that as a common spam word. So to though these may be good, there could be some room for improvement in the clustering models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
